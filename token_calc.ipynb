{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation\n",
    "\n",
    "This code processes a dataset named jameel_ds to clean and organize its contents:\n",
    "\n",
    "- Extracting Duplicated Rows:\n",
    "    It first identifies and stores any duplicated rows in a separate variable, duplicated_rows.\n",
    "\n",
    "- Removing Duplicate Entries:\n",
    "    The code then removes duplicate entries based on the 'Description' column. This means if there are multiple rows with the same description, only the first one is kept.\n",
    "\n",
    "- Dropping Unnecessary Columns:\n",
    "    It removes an irrelevant column (named 'Field1') from the dataset.\n",
    "\n",
    "- Resetting the Index:\n",
    "    The index of the DataFrame is reset, making it sequential and more organized, especially after the removal of some rows.\n",
    "\n",
    "- Renaming Columns:\n",
    "    Column names are changed to more descriptive titles (e.g., 'Text' is renamed to 'Part Name', 'Text1' to 'Part Number', etc.), making the dataset easier to understand.\n",
    "\n",
    "- Cleaning 'Part Number' Column:\n",
    "    Any leading or trailing spaces in the 'Part Number' column are removed for consistency. Additionally, a specific text pattern ('رقم الصنف' followed by any whitespace) is removed from this column to make the part numbers cleaner.\n",
    "\n",
    "- Cleaning 'Price (SAR)' Column:\n",
    "    In the 'Price (SAR)' column, the text pattern 'ريال للقطعة', along with any surrounding whitespace, is removed. This likely transforms the entries in this column to simple numerical values, indicating the price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305 *****\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Part Name</th>\n",
       "      <th>Part Number</th>\n",
       "      <th>Dimensions</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-category</th>\n",
       "      <th>Price (SAR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>طوق اللوحة الخلفية لرأس الاسطوانة</td>\n",
       "      <td>1118235050</td>\n",
       "      <td>\\n                        \\n                  ...</td>\n",
       "      <td>\\n              هذه القطعة مناسبة للموديلات ال...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n              \\n</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>خزان الرادياتير الاحتياطي</td>\n",
       "      <td>1647031050</td>\n",
       "      <td>\\n                        \\n                  ...</td>\n",
       "      <td>\\n              هذه القطعة مناسبة للموديلات ال...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n              \\n</td>\n",
       "      <td>205.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>معدات كوب أسطوانة العجلة الخلفية</td>\n",
       "      <td>0490636060</td>\n",
       "      <td>\\n                        \\n                  ...</td>\n",
       "      <td>\\n              هذه القطعة مناسبة للموديلات ال...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n              \\n</td>\n",
       "      <td>106.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عمود التوازن عمود التوازن , رقم 2</td>\n",
       "      <td>1362275020</td>\n",
       "      <td>\\n                        \\n                  ...</td>\n",
       "      <td>\\n              هذه القطعة مناسبة للموديلات ال...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n              \\n</td>\n",
       "      <td>214.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>المبرد زيت</td>\n",
       "      <td>1571017021</td>\n",
       "      <td>\\n                        \\n                  ...</td>\n",
       "      <td>\\n              هذه القطعة مناسبة للموديلات ال...</td>\n",
       "      <td>التبريد والتسخين</td>\n",
       "      <td>\\n               التبريد والتسخين  / تبريد / م...</td>\n",
       "      <td>1,588.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Part Name Part Number  \\\n",
       "0  طوق اللوحة الخلفية لرأس الاسطوانة  1118235050   \n",
       "1          خزان الرادياتير الاحتياطي  1647031050   \n",
       "2   معدات كوب أسطوانة العجلة الخلفية  0490636060   \n",
       "3  عمود التوازن عمود التوازن , رقم 2  1362275020   \n",
       "4                         المبرد زيت  1571017021   \n",
       "\n",
       "                                          Dimensions  \\\n",
       "0  \\n                        \\n                  ...   \n",
       "1  \\n                        \\n                  ...   \n",
       "2  \\n                        \\n                  ...   \n",
       "3  \\n                        \\n                  ...   \n",
       "4  \\n                        \\n                  ...   \n",
       "\n",
       "                                         Description            Category  \\\n",
       "0  \\n              هذه القطعة مناسبة للموديلات ال...                 NaN   \n",
       "1  \\n              هذه القطعة مناسبة للموديلات ال...                 NaN   \n",
       "2  \\n              هذه القطعة مناسبة للموديلات ال...                 NaN   \n",
       "3  \\n              هذه القطعة مناسبة للموديلات ال...                 NaN   \n",
       "4  \\n              هذه القطعة مناسبة للموديلات ال...   التبريد والتسخين    \n",
       "\n",
       "                                        Sub-category Price (SAR)  \n",
       "0                     \\n              \\n                   27.00  \n",
       "1                     \\n              \\n                  205.50  \n",
       "2                     \\n              \\n                  106.50  \n",
       "3                     \\n              \\n                  214.20  \n",
       "4  \\n               التبريد والتسخين  / تبريد / م...    1,588.80  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jameel_ds = pd.read_csv(\"/home/qparts/Desktop/Jameel Toyota project /Toyota_jameel(1).csv\",on_bad_lines='skip')\n",
    "duplicates = jameel_ds.duplicated(keep=False)\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "# Displaying duplicated rows (optional)\n",
    "duplicated_rows = jameel_ds[duplicates]\n",
    "cleaned_jameel = jameel_ds.drop_duplicates(subset='Text3', keep='first')\n",
    "cleaned_jameel = cleaned_jameel.drop(columns=['Field1'])\n",
    "cleaned_jameel = cleaned_jameel.reset_index(drop=True)\n",
    "cleaned_jameel = cleaned_jameel.rename(columns={'Text':'Part Name', 'Text1':'Part Number', 'Text2':'Dimensions', 'Text3':'Description', 'Field5':'Category', 'Field6':'Sub-category', 'Field7':'Price (SAR)'})\n",
    "cleaned_jameel['Part Number'] = cleaned_jameel['Part Number'].str.strip()\n",
    "cleaned_jameel['Part Number'] = cleaned_jameel['Part Number'].str.replace(r'رقم الصنف\\s*', '', regex=True)\n",
    "cleaned_jameel['Price (SAR)'] = cleaned_jameel['Price (SAR)'].str.replace(r'\\s*ريال\\s*للقطعة', '', regex=True)\n",
    "print(cleaned_jameel['Description'].count(),\"*****\")\n",
    "cleaned_jameel.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Information Extraction and Organization Process\n",
    "\n",
    "This code is designed to extract and organize specific information from text data, particularly focused on vehicle models and their variants. The process includes the following steps:\n",
    "\n",
    "## 1. Text Preparation and Segmentation\n",
    "- Breaks down a large text into smaller chunks to manage text size limits of the processing API.\n",
    "- Uses a specific delimiter and a token limit to ensure manageable sizes of text chunks while maintaining meaningful text segments.\n",
    "\n",
    "## 2. Conversation Handling with an AI Model\n",
    "- Initiates a conversation with an AI model (like GPT-3.5 or GPT-4) for each text chunk.\n",
    "- Tasks the AI to extract information in a structured JSON format, including vehicle model, production start and end dates, and variants.\n",
    "\n",
    "## 3. Response Processing and Error Handling\n",
    "- Collects and checks responses from the AI for errors.\n",
    "- Processes valid responses further; displays error messages if responses are not valid.\n",
    "\n",
    "## 4. Data Extraction and Transformation\n",
    "- Attempts to parse the AI's response into a structured format (JSON).\n",
    "- Transforms this JSON data into a tabular format, creating entries for each vehicle model and its variants.\n",
    "\n",
    "## 5. Data Aggregation and Final Formatting\n",
    "- Further aggregates and organizes the extracted data. Combines variants of each model into a single entry for clarity and conciseness.\n",
    "- Creates a DataFrame from this aggregated data for a structured and tabular presentation.\n",
    "\n",
    "## 6. Output Generation\n",
    "- Saves the DataFrame to an Excel file, providing a structured and easy-to-read document.\n",
    "- Prints the DataFrame for a quick review of the processed data.\n",
    "\n",
    "**Purpose**: The code automates the extraction of detailed vehicle information from text, processes and structures this information, and presents it in an organized, tabular format suitable for further analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model Production Start Production End  \\\n",
      "0               FORTUNER          2005/01        2012/04   \n",
      "1               FORTUNER          2011/07        2021/04   \n",
      "2               FORTUNER          2015/07        2021/04   \n",
      "3               FORTUNER          2020/06        2022/05   \n",
      "4         FORTUNER (SUV)          2021/08        2022/10   \n",
      "5         FORTUNER (SUV)          2022/08             NA   \n",
      "6                  HIACE          2005/08        2007/09   \n",
      "7                  HIACE          2005/08             NA   \n",
      "8                  HIACE          2005/12        2007/09   \n",
      "9                  HILUX           200408         201204   \n",
      "10                 HILUX           201107         202110   \n",
      "11                 HILUX           201505         202105   \n",
      "12                 HILUX           202006         202206   \n",
      "13                 HILUX           202006         202202   \n",
      "14    HILUX (DOUBLE CAB)           202108             NA   \n",
      "15    HILUX (DOUBLE CAB)           202208             NA   \n",
      "16       HILUX (SCB/XTR)           202108             NA   \n",
      "17       HILUX (SCB/XTR)           202208             NA   \n",
      "18  INNOVA/KIJANG INNOVA          2004/08        2016/06   \n",
      "19  INNOVA/KIJANG INNOVA          2011/07        2016/11   \n",
      "\n",
      "                                        Variant Codes  \n",
      "0   2TRFE..TGN51,61..IDSUV;2TRFE..TGN51,61..LA,MA,...  \n",
      "1   2TRFE..TGN51,61; 2TRFE..TGN51,61..CO,GCC,GEN,I...  \n",
      "2   2TRFE..TGN156,16#..AGTSU,EPTSU,IDNSU,MASUV,PKN...  \n",
      "3   2TRFE..TGN156,16#..AGTSU,(L); 2TRFE..TGN156,16...  \n",
      "4   2TRFE..TGN156,16#..AGTP,SAP,(L); 2TRFE..TGN156...  \n",
      "5                               2TRFE..TGN156,166,(L)  \n",
      "6   2TRFE..TRH213..10S..GEN,VIETNAM SPEC; 2TRFE..T...  \n",
      "7                2TRFE..TRH213..VN;2TRFE..TRH203..HRF  \n",
      "8           2TRFE..TRH213..LHD..STD..GEN,VIETNAM SPEC  \n",
      "9   2TRFE..TRH203..MA,TRH223..5F..GL..GCC,IRAQ SPE...  \n",
      "10                2TRFE..TGN16,26,36..ANCOM,LA,SA,(L)  \n",
      "11                  1TRFE,2TRFE..TGN11#,12#,13#,#,(L)  \n",
      "12                   2TRFE..TGN126,128,13#..AD,AS,(L)  \n",
      "13  1TRFE,2TRFE..TGN11#,120,121,126,136..PS,SD,SS,...  \n",
      "14  1TRFE,2TRFE..TGN120,121,126,136..THP,(L); 2TRF...  \n",
      "15                        1TRFE,2TRFE..TGN12#,136,(L)  \n",
      "16  1TRFE,2TRFE..TGN11#,12#..THP,(L); 1TRFE,2TRFE....  \n",
      "17                        1TRFE,2TRFE..TGN11#,12#,(L)  \n",
      "18  1TRFE..TGN40; 1TRFE..TGN40..4FC..(DLX,GL)..(ID...  \n",
      "19                                 1TRFE,2TRFE..TGN4#  \n",
      "********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "# GPT_MODEL = \"gpt-4-0613\"\n",
    "openai.api_key =\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"temperature\": 0.0 ,\"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    if function_call is not None:\n",
    "        json_data.update({\"function_call\": function_call})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n",
    "\n",
    "def execute_function_call(message):\n",
    "    try:\n",
    "        # Attempt to parse the JSON data\n",
    "        data = json.loads(message[\"function_call\"][\"arguments\"])\n",
    "\n",
    "        # Print the data for debugging\n",
    "        print(\"Function call returned data:\", data)\n",
    "\n",
    "        # Access the required_data_json_format key\n",
    "        if \"required_data_json_format\" in data:\n",
    "            info = data[\"required_data_json_format\"]\n",
    "            return info\n",
    "        else:\n",
    "            return \"Key 'required_data_json_format' not found in the function call response.\"\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        return f\"JSON decoding error: {e}\"\n",
    "    except KeyError as e:\n",
    "        return f\"Key error: {e}\"\n",
    "\n",
    "\n",
    "messages = []\n",
    "# def run_conversation(user_input):\n",
    "#     # messages.append({\"role\": \"system\", \"content\": \"Extract the following information from the next user text in JSON format: 'Model', 'Production Start', 'Production End', 'Variant', 'Features', 'Regions', and 'Additional Info'.from the provided text\"})\n",
    "    \n",
    "#     user_message = ({\"role\": \"user\", \"content\":f\"Extract the following information from text in JSON format: 'Model', 'Production Start', 'Production End', 'Variant', 'Features', 'Regions', and 'Additional Info'.from the provided text {user_input}\"})\n",
    "#     messages_to_send = [user_message]\n",
    "\n",
    "#     chat_response = chat_completion_request(messages_to_send)\n",
    "#     if chat_response.status_code != 200:\n",
    "#         print(f\"Error: Unable to get a response from OpenAI. Status code: {chat_response.status_code}, Response: {chat_response.content.decode('utf-8')}\")\n",
    "#     assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "#     # messages.append(assistant_message)\n",
    "#     if assistant_message.get(\"function_call\"):\n",
    "#         info_ext = execute_function_call(assistant_message)\n",
    "#         return info_ext \n",
    "def chunk_text_by_token_limit_and_delimiter(text):\n",
    "    # Specify the maximum number of tokens per chunk\n",
    "    max_tokens = 2000  # Adjust as needed\n",
    "\n",
    "    # Define the delimiter\n",
    "    delimiter = \"من سنةحتى سنةملاحظات على الموديل\"\n",
    "    # Approximate character count per token\n",
    "    chars_per_token = 4\n",
    "\n",
    "    # Calculate max characters per chunk based on token limit\n",
    "    max_chars = max_tokens * chars_per_token\n",
    "\n",
    "    # Split the text using the specified delimiter\n",
    "    parts = re.split(delimiter, text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for part in parts:\n",
    "        # Check if adding this part exceeds the max character limit\n",
    "        if len(current_chunk) + len(part) > max_chars and current_chunk:\n",
    "            # If it does, add the current chunk to the list and start a new one\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = part\n",
    "        else:\n",
    "            # If it doesn't, add the part to the current chunk\n",
    "            current_chunk += (delimiter if current_chunk else \"\") + part\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "def run_conversation(user_input):\n",
    "    # system_message = {\"role\": \"system\", \"content\": \"Extract the following information from the next user text in standard JSON format: 'Model', 'Production Start', 'Production End', and 'Variants'.from the provided text\"}\n",
    "    \n",
    "    # Prepare the message in the format expected by the OpenAI API\n",
    "    user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f'''Please provide the extracted information from this text {user_input}in a flattened JSON format. Each entry should adhere to the following structure:\n",
    "\n",
    "            - 'Model': The name or identifier of the vehicle model.\n",
    "            - 'Production Start': The date when the production of the model started, formatted as YYYY/MM.\n",
    "            - 'Production End': The date when the production of the model ended, formatted as YYYY/MM. If the model is still in production, fill this with 'NA'.\n",
    "            - 'Variants': A list containing only the 'Code' for each variant of the model.\n",
    "\n",
    "            Each variant should be listed as a separate entry. If the 'Production End' is unknown or if the model is still in production, please use 'NA' for that field. For variants, only the unique code that identifies each variant is required, without the need for a description of the specifications or features.\n",
    "                '''\n",
    "            }\n",
    "    messages_to_send = [user_message]\n",
    "\n",
    "    # Assuming 'chat_completion_request' is a function that makes the API call\n",
    "    chat_response = chat_completion_request(messages_to_send)\n",
    "    \n",
    "    if chat_response.status_code != 200:\n",
    "        return {\n",
    "            \"error\": f\"Unable to get a response from OpenAI. Status code: {chat_response.status_code}\",\n",
    "            \"response\": chat_response.content.decode('utf-8')\n",
    "        }\n",
    "\n",
    "    # Parse the assistant's message from the response\n",
    "    assistant_message = chat_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    # print('Assistant :' , assistant_message)\n",
    "    \n",
    "    # Assuming the assistant_message is a string that contains the JSON-formatted information\n",
    "    # Convert the JSON string back to a dictionary (if it's not already a dictionary)\n",
    "    try:\n",
    "        info_ext = json.loads(assistant_message)\n",
    "        return info_ext\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Assistant :\",assistant_message)\n",
    "        return {\"error\": \"Failed to parse the assistant's response as JSON.\"}\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def convert_json_to_dataframe(json_str):\n",
    "    # Parse the JSON string into a Python object (list of dictionaries)\n",
    "    data = json.loads(json_str)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    return pd.DataFrame(data)\n",
    "def reduce_spaces(text):\n",
    "    # Remove extra spaces and line breaks\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_chunks = chunk_text_by_token_limit_and_delimiter(text)\n",
    "messages = []\n",
    "combined_responses = []\n",
    "for chunk in text_chunks:\n",
    "    response = run_conversation(chunk)\n",
    "        # Check if response is not empty and the first item is not an error\n",
    "     # Check if the response is a list and is not empty\n",
    "    if isinstance(response, list) and response:\n",
    "        # Check if the first item in the list contains an error key\n",
    "        if isinstance(response[0], dict) and \"error\" not in response[0]:\n",
    "            combined_responses.extend(response)\n",
    "        else:\n",
    "            print(\"Error in chunk response:\", response[0])\n",
    "    else:\n",
    "        print(\"Unexpected response format or empty response:\", response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduced_text  = reduce_spaces(text)\n",
    "# json_output = run_conversation(reduced_text)\n",
    "# print(json_output)\n",
    "\n",
    "flattened_data = []\n",
    "for response in combined_responses:\n",
    "    if isinstance(response, dict):\n",
    "        entries = [response]\n",
    "    elif isinstance(response, list):\n",
    "        entries = response\n",
    "    else:\n",
    "        print(\"Unexpected response format. Skipping this response.\")\n",
    "        continue\n",
    "\n",
    "    for entry in entries:\n",
    "        if isinstance(entry, dict):\n",
    "            model = entry.get('Model', 'NA')\n",
    "            prod_start = entry.get('Production Start', 'NA')\n",
    "            prod_end = entry.get('Production End', 'NA')\n",
    "            variants = entry.get('Variants', [])\n",
    "\n",
    "            if all(isinstance(variant, str) for variant in variants):\n",
    "                for variant in variants:\n",
    "                    variant_data = {\n",
    "                        'Model': model,\n",
    "                        'Production Start': prod_start,\n",
    "                        'Production End': prod_end,\n",
    "                        'Variant Code': variant\n",
    "                    }\n",
    "                    flattened_data.append(variant_data)\n",
    "            else:\n",
    "                print(f\"Variants for model {model} are not in the expected string format.\")\n",
    "        else:\n",
    "            print(\"Entry is not a dictionary. Skipping this entry.\")\n",
    "\n",
    "# Group and combine data\n",
    "grouped_data = defaultdict(set)\n",
    "\n",
    "for entry in flattened_data:\n",
    "    key = (entry['Model'], entry['Production Start'], entry['Production End'])\n",
    "    grouped_data[key].add(entry['Variant Code'])\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "for (model, prod_start, prod_end), variants in grouped_data.items():\n",
    "    combined_data.append({\n",
    "        'Model': model,\n",
    "        'Production Start': prod_start,\n",
    "        'Production End': prod_end,\n",
    "        'Variant Codes': '; '.join(sorted(variants))\n",
    "    })\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(combined_data)\n",
    "\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(\"vehicle_info.xlsx\", index=False)\n",
    "\n",
    "# Print DataFrame\n",
    "print(df)\n",
    "\n",
    "print(\"********************************************************************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
